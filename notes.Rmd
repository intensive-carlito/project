---
title: "Air BnB à Paris"
author:
  - Charles Picard
  - Mathieu Giardini
  - David Boucher
#abstract: "J'ai un appartement à Paris que je voudrais mettre en location sur Air BnB. A quel prix pourrais-je le louer ?"

output:
  slidy_presentation: default
  beamer_presentation: default
  ioslides_presentation: default
---

---

# Les données Air BnB

* Le site `http://insideairbnb.com/` propose des fichiers _csv_ contenant les données du site Air BnB. Nous avons donc récupéré un fichier `listings.csv` contenant environ 60 000 appartements à Paris.

* Nous avons nettoyé ce tableau :
  * Filtrage sur le code postal afin de n'avoir que du 75 ; vérification du code postal afin qu'il n'ait que 5 chiffres
  * Conversions de certaines valeurs avec renommage des champs si nécessaire.
  * Conversion des données de date en date au sens R.
  * Les appartements dont les prix ne sont pas fournis ou hors norme sont supprimés.

---

# Les monuments de Paris

* Nous nous sommes dit que les locataires étaient peut-être intéressés par une proximité entre leur location et les monuments parisiens.

  * Un peu de scrapbooking sur le site http://monumentsdeparis.net/ permet de récupérer les monuments parisiens ainsi que leurs adresses.
  * L'utilisation de l'API REST de `adresse.data.gouv.fr` permet de récupérer les adresses dans un format harmonisé avec les longitudes/latitudes.
  * L'étape suivante a été de déterminer pour chaque logement le nombre de monuments à moins de 100m, 200m, 500m et 1km.

---

# Les gares et stations de métro/RER

* Comme pour les monuments, il est peut-être intéressant pour un locataire de chercher à ce que son logement soit proche de gares/stations de métro.

  * Nous avons récupéré le fichier `accessibilite-des-gares-et-stations-metro-et-rer-ratp.csv` sur `https://www.data.gouv.fr/en/datasets/accessibilite-des-gares-et-stations-de-metro-et-rer-ratp-1/`.
  * Les retraitements de ce fichier consistent en :
    * Ne garder que les adresses du département 75.
    * Supprimer les doublons
    * Séparer la longitude et la latitude en deux colonnes distinctes.
  * Nous avons ensuite pu déterminer le nombre de stations/gares à moins de 100m, 200m, 500m et 1km.

---

# Les avis des locataires sur les logements

* Nous avons récupéré un fichier `reviews.csv` sur `http://insideairbnb.com/` contenant les revues des clients (environ 1 000 000 de lignes).
* Nous avons tenté d'en extraire des informations mais la variété des langues a rendu son exploitation assez difficile.
* De plus, notre but étant d'estimer le prix d'une location que l'on aurait à proposer, il nous a semblé difficile d'avoir des revues dès le début.
* Cette piste a donc été écartée.

---

# Fabrication des quartiers

* Nous avons téléchargé les quartiers de Paris sur le site https://opendata.paris.fr/explore/dataset/quartier_paris/export/.
* A partir de nos données Air Bnb, nous avons aussi récupéré les id, latitudes, longitudes des logements.

* Ces deux sources ont été importées dans le logiciel QGIS. Son outil d'intersection nous a alors permis de déterminer à quels quartiers appartiennent chacune des locations.
* Après cette opération, nous avons exporté le résultat dans le fichier `quartiers.csv`.

![QGIS](./images/qgis.png){width=500px}

---

# Premiers modèles

* Une fois toutes nos données regroupées, nous avons commencé par expérimenter quelques modèles :
  * Régression linéaire
  * Régression log linéaire
  * Random Forest avec 500 arbres.

---

# Densité des prix

* Les prix sont à valeurs dans l'intervalle $[0;+\infty[$. Il est probable que la distribution des prix ne suit pas une loi normale...
* Si on trace la densité des prix, on obtient :

![Densite-prix](./images/densite-prix.png)

---

* Traçons maintenant la densité du $\log$ des prix. On obtient alors :

![Densite-prix](./images/densite-log-prix.png)

* Courbe qui ressemble davantage à une gaussienne.
* Plutôt que d'utiliser une régression linéaire pour donner le prix, il semble plus judicieux d'utiliser une régression $\log$ linéaire (régression de Poisson).

---

# Quelques résultats sur ces premiers modèles


```{r functions,echo=F}
tabl <- data.frame(Indicateur = c("Average IB error", "$\\sigma$ IB error (RMSE)", "$R^2_a$", "Average OB error", "$\\sigma$ OB error (RMSE)"),
                   LM=c(0, 43.41, 0.52, 0.51, 36.75), GLM=c(1.35, 32.32, 0.73, 0.68, 44.18),
                   RF=c(0.27, 37.79, 0.64, 0.51, 36.75))
knitr::kable(tabl)
```

---

# Choix des variables

A partir du random forest, nous avons déterminé l'importance des variables avec deux méthodes :

* Mean decrease accuracy
* Mean decrease GINI

On a obtenu les graphiques suivants...

---

# Importance des variables

![Importance des variables (Mean decrease accuracy)](./images/Mean_De_ACC.png){width=500px}

![Importance des variables (Mean Decrease GINI](./images/Mean_De_GINI.png){width=500px}

---

# Sélection des variables

Les 9 variables les plus importantes (par croisement des deux) que nous avons choisies :

* bedrooms
* accommodates
* l_qu
* zipcode
* host_total_listings_count
* bathrooms
* delai_inscription
* nb_amen
* summary_l

---

# Régressions linéaires avec les 9 variables sélectionnées

* Deux régressions sont faites :
  * Une première avec les 9 variables :
    * bedrooms, accommodates, l_qu, zipcode, host_total_listings_count, bathrooms, delai_inscription, nb_amen, summary_l
  * Une seconde avec une application du critère AIC qui a réduit les variables à 7 :
    * bedrooms, accomodates, l_qu, zipcode, host_total_listings_count, bathrooms, nb_amen, summary_l (beds et delai_inscription sont supprimés)

```{r resultLM,echo=F}
tabl <- data.frame(Indicateur = c("Average IB error", "$\\sigma$ IB error (RMSE)", "$R^2_a$", "Average OB error", "$\\sigma$ OB error (RMSE)"),
                   LM1=c(4.03412137823296e-15, 44.7820916530383, 0.48827020151371, 0.625523131513831, 45.7526415862631),
                   LM2=c(1.24463403514636e-14, 44.7824626353489, 0.48826172296872, 0.623229361866199, 45.7536758787619))
knitr::kable(tabl)
```

---

# Régressions log linéaires avec les 9 variables sélectionnées

* Deux régressions sont faites :
  * Une première avec les 9 variables :
    * bedrooms, accommodates, l_qu, zipcode, host_total_listings_count, bathrooms, delai_inscription, nb_amen, summary_l
  * Une seconde avec une application du critère AIC qui a réduit les variables à 7 :
    * bedrooms, accomodates, l_qu, zipcode, host_total_listings_count, bathrooms, nb_amen, summary_l (beds et delai_inscription sont supprimés)

```{r resultGLM,echo=F}
tabl <- data.frame(Indicateur = c("Average IB error", "$\\sigma$ IB error (RMSE)", "$R^2_a$", "Average OB error", "$\\sigma$ OB error (RMSE)"),
                   GLM1=c(1.18192082968798, 4.38022431314205, 0.995069715042558, 0.73474059255429, 47.4262325330374),
                   GLM2=c(1.18192030421104, 4.37998251476438, 0.995070691872251, 0.734734397497239, 47.4262531471582))
knitr::kable(tabl)
```

---

# Random forest

* Une random forest a été faite avec 500 arbres et les paramètres par défaut de R. Nous sommes toujours sur les variables :
    * bedrooms, accommodates, l_qu, zipcode, host_total_listings_count, bathrooms, delai_inscription, nb_amen, summary_l

```{r resultRF,echo=F}
tabl <- data.frame(Indicateur = c("Average IB error", "$\\sigma$ IB error (RMSE)", "$R^2_a$", "Average OB error", "$\\sigma$ OB error (RMSE)"),
                   RF=c(-0.01293537, 37.79030141, 0.63684109, 0.24555499, 36.72446884))
knitr::kable(tabl)
```
---

# Modèle de gradient boosting d'arbre de régression

* 3 modèles (réalisés avec nos 9 variables) ont été comparés pour tester les différents paramètres de la fonction *gbm()* :
  * Un premier avec les paramètres suivants :
    * shrinkage = 0.01 : pas lors de la descente du gradient
    * interaction.depth = 1 : Nombre de noeuds de scission de chaque arbre de regression (donc 2 noeuds terminaux)
  * Un second avec les paramètres suivants :
    * shrinkage = 0.1 : descente plus rapide que pour le modèle 1
    * interaction.depth = 1 : Meme nombre de noeuds de scission que pour le modèle 1
* Un troisième avec les paramètres suivants :
    * shrinkage = 0.01 : pas de descente identique que pour le modèle 1
    * interaction.depth = 2 : nombre de noeuds de scission plus important que pour le modèle 1, ce qui signifie des arbres de régression plus complexes

---

# Résultats de gradient boosting

```{r resultBoosting,echo=F}
tabl <- data.frame(Indicateur = c("Average IB error", "$\\sigma$ IB error (RMSE)", "$R^2_a$", "Average OB error", "$\\sigma$ OB error (RMSE)"),
                   GB1=c(0.0132347150777637, 40.7149516080917, 0.577000548690167, 0.529526398684977, 40.7678481983617),
                   GB2=c(0.0182772855451852, 40.4881667247804, 0.58169969284445, 0.515773710719386, 40.6450388471167),
                   GB3=c(0.00907893793537792, 38.4621048461559, 0.622516430636094, 0.446708052219003, 39.5790870564872))
knitr::kable(tabl)
```

---

# Comparison des 3 types de modèles avec les 9 variables sélectionnées

* 3 modèles finaux ont été sélectionnés :
  * Un modèle de **régression de Poisson** (GLM) pour prendre en compte la normalité du logarithme des prix
  * Un modèle de **forêts aléatoires** calibré avec 500 arbres et un paramètre *mtry* égal à 3
  * Un modèle de **gradient boosting d'arbre de régression** calibré avec un **shrinkage** de 0.01 et un *interaction.depth* de 2
  
# Résumé des résultats

```{r result3Mod,echo=F}
tabl <- data.frame(Indicateur = c("Average IB error", "$\\sigma$ IB error (RMSE)", "$R^2_a$", "Average OB error", "$\\sigma$ OB error (RMSE)"),
                   GLM2=c(1.18192030421104, 4.37998251476438, 0.995070691872251, 0.734734397497239, 47.4262531471582),
                   RF1=c(0.248075878500565, 39.5490623950428, 0.600863513782569, 0.337663433198099, 39.2330848198853),
                   GB3=c(0.00907893793537792, 38.4621048461559, 0.622516430636094, 0.446708052219003, 39.5790870564872))
knitr::kable(tabl)
```

Nous avons incorporé dans notre application Shiny le prix pour chacun des 3 modèles.

